{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35150174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from model_zoo import get_model\n",
    "from dataset_zoo import VG_Relation, VG_Attribution, COCO_Order, Flickr30k_Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d2d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your data root directory below. We'll download VG-Relation and VG-Attribution images here. \n",
    "# Will be a 1GB zip file (a subset of GQA).\n",
    "root_dir=\"~/.cache\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = get_model(model_name=\"openai-clip:ViT-B/32\", device=\"cpu\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b942fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Get the VG-R dataset\n",
    "vgr_dataset = VG_Relation(image_preprocess=preprocess, download=False, root_dir=root_dir)\n",
    "vgr_dataset.dataset = vgr_dataset.dataset[:1000]\n",
    "vgr_dataset.all_relations = vgr_dataset.all_relations[:1000]\n",
    "vgr_loader = DataLoader(vgr_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(len(vgr_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(vgr_dataset[0][\"image_options\"][0].shape)\n",
    "for ex in vgr_loader:\n",
    "    break\n",
    "    if len(ex['image_options']) > 1:\n",
    "        print(f\"length of options: {len(ex['image_options'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/projects/vision-language-models-are-bows/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b031b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "normalize = T.Normalize(\n",
    "        mean=[-m / s for m, s in zip([0.485, 0.456, 0.406],\n",
    "                                    [0.229, 0.224, 0.225])],\n",
    "        std=[1/s for s in [0.229, 0.224, 0.225]]\n",
    "    )\n",
    "\n",
    "to_pil = T.ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "def get_retrieval_scores_batched(model, loader):\n",
    "    tqdm_loader = tqdm(loader)\n",
    "    tqdm_loader.set_description(\"Computing retrieval scores\")\n",
    "    image_scores = []\n",
    "    no_image_scores = []\n",
    "    image_correct = []\n",
    "    no_image_correct = []\n",
    "    probability_increase = []\n",
    "    \n",
    "    for batch in tqdm_loader:\n",
    "        image = batch['image_options'][0][0]\n",
    "        image = normalize(image)\n",
    "        image = to_pil(image)\n",
    "\n",
    "        image_path = \"cur_image.png\"\n",
    "        image.save(image_path)\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            encoded_image = base64.b64encode(f.read())\n",
    "        encoded_image = encoded_image.decode(\"utf-8\")\n",
    "\n",
    "        with_image_perplexities = []\n",
    "        no_image_perplexities= []\n",
    "        with_image_log_probs = []\n",
    "        no_image_log_probs = []\n",
    "\n",
    "        for add_image in [False, True]:\n",
    "            for i in range(2):\n",
    "                text = batch['caption_options'][i][0]\n",
    "                messages = []\n",
    "                if add_image:\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"image\",\n",
    "                                    \"image\": \"file://\" + image_path\n",
    "                                },\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": text\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                text = processor.apply_chat_template(messages, tokenize=False, add_generation=False)\n",
    "                tokenized_text = processor.apply_chat_template(messages[-1:], tokenize=True, add_special_tokens=False, return_tensors=\"pt\")\n",
    "                tokenized_text = tokenized_text.to(model.device)\n",
    "\n",
    "                image_inputs, video_inputs = process_vision_info(messages)\n",
    "                inputs = processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                inputs = inputs.to(model.device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                log_probs = logits.log_softmax(dim=-1)[:,-tokenized_text.shape[1]:-1]\n",
    "                \n",
    "                selected_log_probs = log_probs.gather(dim=-1, index=tokenized_text[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "                perplexity = (-selected_log_probs.mean()).exp().item()\n",
    "                if add_image:\n",
    "                    with_image_perplexities.append(perplexity)\n",
    "                    with_image_log_probs.append(selected_log_probs.sum())\n",
    "                else:\n",
    "                    no_image_perplexities.append(perplexity)\n",
    "                    no_image_log_probs.append(selected_log_probs.sum())\n",
    "\n",
    "\n",
    "        # since the correct caption is always the second one\n",
    "        with_image_correct = with_image_perplexities[1] < with_image_perplexities[0]\n",
    "        no_image_correct = no_image_perplexities[1] < no_image_perplexities[0]\n",
    "        ratio_image = with_image_log_probs[1] - with_image_log_probs[0]\n",
    "        ratio_no_image = no_image_log_probs[1] - no_image_log_probs[0]\n",
    "        probability_increase = (ratio_image - ratio_no_image).exp().item()\n",
    "\n",
    "        image_scores.append([1/ ppl for ppl in with_image_perplexities])\n",
    "        no_image_scores.append([1/ ppl for ppl in no_image_perplexities])\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"with image perplexities: {with_image_perplexities}\")\n",
    "        print(f\"no image perplexities: {no_image_perplexities}\")\n",
    "\n",
    "        print(f\"ratio with image: {ratio_image.exp().item()}\")\n",
    "        print(f\"ratio without image: {ratio_no_image.exp().item()}\")\n",
    "        print(f\"with image correct: {with_image_correct}\")\n",
    "        print(f\"no image correct: {no_image_correct}\")\n",
    "        print(f\"probability increase factor: {probability_increase}\")\n",
    "        print(batch[\"caption_options\"])\n",
    "        print()\n",
    "        \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"image_scores\": image_scores,\n",
    "        \"no_image_scores\": no_image_scores,\n",
    "        \"image_correct\": image_correct,\n",
    "        \"no_image_correct\": no_image_correct,\n",
    "        \"probability_increase\": probability_increase\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores: 100%|██████████| 100/100 [00:15<00:00,  6.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Compute the scores for each test case\n",
    "vgr_output = get_retrieval_scores_batched(model, vgr_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00465b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgr_image_scores = np.array(vgr_output[\"image_scores\"])[:, None, :]\n",
    "vgr_no_image_scores = np.array(vgr_output[\"no_image_scores\"])[:, None, :]\n",
    "vgr_image_correct = vgr_output[\"image_correct\"]\n",
    "vgr_no_image_correct = vgr_output[\"no_image_correct\"]\n",
    "vgr_probability_increase = vgr_output[\"probability_increase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "#print(vgr_dataset[0])\n",
    "for idx in range(0, 20):\n",
    "    continue\n",
    "    vgr_dataset[0][\"image_options\"][0].shape\n",
    "    print(vgr_dataset[idx][\"caption_options\"])\n",
    "    image = normalize(vgr_dataset[idx][\"image_options\"][0])\n",
    "    image = to_pil(image)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG-Relation Macro Accuracy with image: 0.839716610549944\n",
      "VG-Relation Macro Accuracy with image: 0.6268799102132435\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vgr_records = vgr_dataset.evaluate_scores(vgr_image_scores)\n",
    "symmetric = ['adjusting', 'attached to', 'between', 'bigger than', 'biting', 'boarding', 'brushing', 'chewing', 'cleaning', 'climbing', 'close to', 'coming from', 'coming out of', 'contain', 'crossing', 'dragging', 'draped over', 'drinking', 'drinking from', 'driving', 'driving down', 'driving on', 'eating from', 'eating in', 'enclosing', 'exiting', 'facing', 'filled with', 'floating in', 'floating on', 'flying', 'flying above', 'flying in', 'flying over', 'flying through', 'full of', 'going down', 'going into', 'going through', 'grazing in', 'growing in', 'growing on', 'guiding', 'hanging from', 'hanging in', 'hanging off', 'hanging over', 'higher than', 'holding onto', 'hugging', 'in between', 'jumping off', 'jumping on', 'jumping over', 'kept in', 'larger than', 'leading', 'leaning over', 'leaving', 'licking', 'longer than', 'looking in', 'looking into', 'looking out', 'looking over', 'looking through', 'lying next to', 'lying on top of', 'making', 'mixed with', 'mounted on', 'moving', 'on the back of', 'on the edge of', 'on the front of', 'on the other side of', 'opening', 'painted on', 'parked at', 'parked beside', 'parked by', 'parked in', 'parked in front of', 'parked near', 'parked next to', 'perched on', 'petting', 'piled on', 'playing', 'playing in', 'playing on', 'playing with', 'pouring', 'reaching for', 'reading', 'reflected on', 'riding on', 'running in', 'running on', 'running through', 'seen through', 'sitting behind', 'sitting beside', 'sitting by', 'sitting in front of', 'sitting near', 'sitting next to', 'sitting under', 'skiing down', 'skiing on', 'sleeping in', 'sleeping on', 'smiling at', 'sniffing', 'splashing', 'sprinkled on', 'stacked on', 'standing against', 'standing around', 'standing behind', 'standing beside', 'standing in front of', 'standing near', 'standing next to', 'staring at', 'stuck in', 'surrounding', 'swimming in', 'swinging', 'talking to', 'topped with', 'touching', 'traveling down', 'traveling on', 'tying', 'typing on', 'underneath', 'wading in', 'waiting for', 'walking across', 'walking by', 'walking down', 'walking next to', 'walking through', 'working in', 'working on', 'worn on', 'wrapped around', 'wrapped in', 'by', 'of', 'near', 'next to', 'with', 'beside', 'on the side of', 'around']\n",
    "df = pd.DataFrame(vgr_records)\n",
    "df = df[~df.Relation.isin(symmetric)]\n",
    "print(f\"VG-Relation Macro Accuracy with image: {df.Accuracy.mean()}\")\n",
    "\n",
    "\n",
    "# Evaluate the macro accuracy\n",
    "vgr_records = vgr_dataset.evaluate_scores(vgr_no_image_scores)\n",
    "df = pd.DataFrame(vgr_records)\n",
    "df = df[~df.Relation.isin(symmetric)]\n",
    "print(f\"VG-Relation Macro Accuracy with image: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/projects/vision-language-models-are-bows/.venv/lib/python3.12/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=13tWvOrNOLHxl3Rm9cR3geAdHx2qR3-Tw\n",
      "To: /home/loic/projects/vision-language-models-are-bows/notebooks/~/.cache/visual_genome_attribution.json\n",
      "100%|██████████| 8.71M/8.71M [00:00<00:00, 18.8MB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2_5_VLForConditionalGeneration' object has no attribute 'get_retrieval_scores_batched'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m vga_loader \u001b[38;5;241m=\u001b[39m DataLoader(vga_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the scores for each test case\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vga_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_retrieval_scores_batched\u001b[49m(vga_loader)\n",
      "File \u001b[0;32m~/projects/vision-language-models-are-bows/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2_5_VLForConditionalGeneration' object has no attribute 'get_retrieval_scores_batched'"
     ]
    }
   ],
   "source": [
    "# Get the VG-A dataset\n",
    "vga_dataset = VG_Attribution(image_preprocess=preprocess, download=True, root_dir=root_dir)\n",
    "vga_loader = DataLoader(vga_dataset, batch_size=16, shuffle=False)\n",
    "# Compute the scores for each test case\n",
    "vga_scores = model.get_retrieval_scores_batched(vga_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08549216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vga_records = vga_dataset.evaluate_scores(vga_scores)\n",
    "df = pd.DataFrame(vga_records)\n",
    "print(f\"VG-Attribution Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c974373",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(400, 1000, 50):\n",
    "    print(vga_dataset[idx][\"caption_options\"])\n",
    "    normalize = T.Normalize(\n",
    "        mean=[-m / s for m, s in zip([0.485, 0.456, 0.406],\n",
    "                                    [0.229, 0.224, 0.225])],\n",
    "        std=[1/s for s in [0.229, 0.224, 0.225]]\n",
    "    )\n",
    "\n",
    "    to_pil = T.ToPILImage()\n",
    "    image = normalize(vga_dataset[idx][\"image_options\"][0])\n",
    "    image = to_pil(image)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_zoo import get_dataset\n",
    "coco_dataset = get_dataset(\"COCO_Order\", image_preprocess=preprocess, download=True, root_dir=\"./coco_data\")\n",
    "coco_loader = DataLoader(coco_dataset, batch_size=16, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
